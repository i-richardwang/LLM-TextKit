import streamlit as st
import pandas as pd
import uuid
import asyncio
from typing import List

from frontend.ui_components import show_sidebar, show_footer, apply_common_styles, display_project_info
from backend.classification.content_analysis_workflow import (
    TextContentAnalysisWorkflow,
)
from backend.classification.content_analysis_core import (
    ContentAnalysisInput,
    ContentAnalysisResult,
)

# Apply custom styles
apply_common_styles()

# Display sidebar
show_sidebar()

# Initialize text classification workflow
workflow = TextContentAnalysisWorkflow()


# Initialize session state
def initialize_session_state():
    """Initialize Streamlit session state with default values."""
    default_states = {
        "classification_results": None,
        "df": None,
        "filtered_df": None,
        "context": "",
        "session_id": str(uuid.uuid4()),
        "is_processing": False,
    }
    for key, value in default_states.items():
        if key not in st.session_state:
            st.session_state[key] = value


initialize_session_state()


def display_classification_result(result: ContentAnalysisResult):
    """Display analysis result as a table"""
    df = pd.DataFrame(
        {
            "Validity": [result.validity],
            "Sentiment Class": [result.sentiment_class],
            "Sensitive Info": [result.sensitive_info],
        }
    )
    st.table(df)


async def batch_classify(texts: List[str], context: str, progress_bar, status_area):
    total_texts = len(texts)
    workflow = TextContentAnalysisWorkflow()
    results = []

    async def process_batch(batch):
        batch_results = await workflow.async_batch_analyze(
            batch, context, st.session_state.session_id, max_concurrency=3
        )
        results.extend(batch_results)
        return len(batch_results)

    batch_size = 3
    for i in range(0, total_texts, batch_size):
        batch = texts[i : i + batch_size]
        processed_count = await process_batch(batch)

        # Update progress bar and status information
        progress = (i + processed_count) / total_texts
        progress_bar.progress(progress)
        status_message = f"Processed: {i + processed_count}/{total_texts}"
        status_area.info(status_message)

        # Add small delay to allow UI update
        await asyncio.sleep(0.05)

    return [result.model_dump() for result in results]


def display_info_message():
    """Display information message for sentiment analysis and annotation tool."""
    st.info(
        """
    æƒ…æ„Ÿåˆ†æä¸æ ‡æ³¨åŠŸèƒ½ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å¤„ç†æŠ€æœ¯ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿåˆ†æå’Œåˆ†ç±»å¤§é‡æ–‡æœ¬æ•°æ®ã€‚
    
    ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
    - æ–‡æœ¬æœ‰æ•ˆæ€§åˆ¤æ–­
    - æƒ…æ„Ÿå€¾å‘åˆ†æ
    - æ˜¯å¦æ•æ„Ÿä¿¡æ¯è¯†åˆ«
    
    é€‚ç”¨äºå„ç±»éœ€è¦å¿«é€Ÿç†è§£å’Œåˆ†ç±»å¤§é‡æ–‡æœ¬æ•°æ®çš„åœºæ™¯ï¼Œå¦‚å®¢æˆ·åé¦ˆåˆ†æã€ç¤¾äº¤åª’ä½“ç›‘æ§ç­‰ã€‚
    """
    )


def display_workflow():
    """Display workflow for sentiment analysis and annotation."""
    with st.expander("ğŸ“‹ æŸ¥çœ‹æƒ…æ„Ÿåˆ†æä¸æ ‡æ³¨å·¥ä½œæµç¨‹", expanded=False):

        with st.container(border=True):
            st.markdown(
                """
            1. **æ•°æ®å‡†å¤‡**: 
               - è¾“å…¥å•æ¡æ–‡æœ¬æˆ–ä¸Šä¼ CSVæ–‡ä»¶
               - æŒ‡å®šæ–‡æœ¬çš„ä¸Šä¸‹æ–‡æˆ–ä¸»é¢˜
               - å®šä¹‰åˆ†ç±»æ ‡ç­¾åˆ—è¡¨
            
            2. **æ–‡æœ¬åˆ†ç±»**:
               - ç³»ç»Ÿè‡ªåŠ¨åˆ¤æ–­æ–‡æœ¬æœ‰æ•ˆæ€§
               - åˆ†ç±»æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘
               - è¯†åˆ«å¯èƒ½çš„æ•æ„Ÿä¿¡æ¯
            
            3. **ç»“æœå±•ç¤º**:
               - æ˜¾ç¤ºæ¯æ¡æ–‡æœ¬çš„åˆ†ç±»ç»“æœ
               - å¯¹äºæ‰¹é‡å¤„ç†ï¼Œä»¥è¡¨æ ¼å½¢å¼å±•ç¤ºæ‰€æœ‰ç»“æœ
            
            4. **ç»“æœå¯¼å‡º**:
               - æä¾›åˆ†ç±»ç»“æœçš„CSVä¸‹è½½é€‰é¡¹
               - ä¾¿äºè¿›ä¸€æ­¥åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
            
            5. **è¿­ä»£ä¼˜åŒ–**:
               - æ ¹æ®åˆ†ç±»ç»“æœè°ƒæ•´ä¸Šä¸‹æ–‡æˆ–æ ‡ç­¾
               - é‡æ–°è¿è¡Œåˆ†ç±»ä»¥æé«˜å‡†ç¡®æ€§
            """
            )


def main():
    st.title("ğŸ·ï¸ æƒ…æ„Ÿåˆ†æä¸æ ‡æ³¨")

    display_info_message()
    
    # Display project information
    display_project_info()
    
    st.markdown("---")
    
    display_workflow()

    st.markdown("## æƒ…æ„Ÿåˆ†æä¸æ ‡æ³¨")
    with st.container(border=True):
        st.session_state.context = st.text_input(
            "è¯·è¾“å…¥æ–‡æœ¬ä¸Šä¸‹æ–‡æˆ–ä¸»é¢˜",
            value=st.session_state.context,
            placeholder="ä¾‹å¦‚ï¼šå‘˜å·¥è°ƒç ”",
        )

        tab1, tab2 = st.tabs(["ç›´æ¥è¾“å…¥", "ä¸Šä¼ CSVæ–‡ä»¶"])

        with tab1:
            with st.form("single_classification_form", border=False):
                text_to_classify = st.text_area("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬", height=150)
                submit_button = st.form_submit_button("åˆ†æ")

                if submit_button:
                    if text_to_classify and st.session_state.context:
                        st.session_state.session_id = str(
                            uuid.uuid4()
                        )  # Generate new session_id for single classification task
                        with st.spinner("æ­£åœ¨åˆ†æ..."):
                            input_data = ContentAnalysisInput(
                                text=text_to_classify,
                                context=st.session_state.context,
                            )
                            result = workflow.analyze_text(
                                input_data, st.session_state.session_id
                            )
                        st.session_state.classification_results = result
                    else:
                        st.warning("è¯·è¾“å…¥æ–‡æœ¬å’Œä¸Šä¸‹æ–‡")

        with tab2:
            uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type="csv")
            
            if st.button("ğŸ“¥ å¯¼å…¥ç¤ºä¾‹æ•°æ®"):
                try:
                    demo_path = "data/uploads/demo_texts.csv"
                    st.session_state.df = pd.read_csv(demo_path)
                    st.success("âœ… å·²åŠ è½½ç¤ºä¾‹æ•°æ®")
                except Exception as e:
                    st.error(f"âŒ åŠ è½½ç¤ºä¾‹æ•°æ®å¤±è´¥ï¼š{str(e)}")
            
            # Process uploaded file
            if uploaded_file is not None:
                try:
                    st.session_state.df = pd.read_csv(uploaded_file)
                except Exception as e:
                    st.error(f"å¤„ç†CSVæ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
            
            # If data is loaded (either via upload or sample data), display operation interface
            if st.session_state.df is not None:
                st.write("é¢„è§ˆä¸Šä¼ çš„æ•°æ®ï¼š")
                st.dataframe(st.session_state.df)

                st.session_state.text_column = st.selectbox(
                    "é€‰æ‹©åŒ…å«è¦åˆ†ææ–‡æœ¬çš„åˆ—", st.session_state.df.columns
                )

                if st.button("å¼€å§‹æ‰¹é‡åˆ†æ"):
                    if st.session_state.context:
                        st.session_state.session_id = str(
                            uuid.uuid4()
                        )  # Generate new session_id for entire batch task
                        st.session_state.filtered_df = st.session_state.df[
                            [st.session_state.text_column]
                        ].copy()
                        st.session_state.current_batch_index = 0
                        st.session_state.total_rows = len(
                            st.session_state.filtered_df
                        )
                        st.session_state.progress = 0
                        st.session_state.is_processing = True
                    else:
                        st.warning("è¯·è¾“å…¥ä¸Šä¸‹æ–‡")

    if st.session_state.is_processing:
        st.markdown("## æ‰¹é‡åˆ†æè¿›åº¦")
        with st.container(border=True):
            total_rows = len(st.session_state.filtered_df)

            progress_bar = st.progress(0)
            status_area = st.empty()

            texts_to_classify = st.session_state.filtered_df[st.session_state.text_column].tolist()

            with st.spinner("æ­£åœ¨æ‰¹é‡åˆ†æ..."):
                results = asyncio.run(
                    batch_classify(
                        texts_to_classify,
                        st.session_state.context,
                        progress_bar,
                        status_area,
                    )
                )

            for i, result in enumerate(results):
                st.session_state.filtered_df.loc[i, "Validity"] = result["validity"]
                st.session_state.filtered_df.loc[i, "Sentiment Class"] = result[
                    "sentiment_class"
                ]
                st.session_state.filtered_df.loc[i, "Sensitive Info"] = result[
                    "sensitive_info"
                ]

            st.success("æ‰¹é‡åˆ†æå®Œæˆï¼")
            st.session_state.classification_results = st.session_state.filtered_df
            st.session_state.is_processing = False

    # Display classification results
    if st.session_state.classification_results is not None:
        st.markdown("## åˆ†æç»“æœ")
        with st.container(border=True):
            if isinstance(
                st.session_state.classification_results, ContentAnalysisResult
            ):
                # Single text classification result
                display_classification_result(st.session_state.classification_results)
            elif isinstance(st.session_state.classification_results, pd.DataFrame):
                # Batch classification results
                st.dataframe(st.session_state.classification_results)

                # Provide download option
                csv = st.session_state.classification_results.to_csv(
                    index=False
                ).encode("utf-8-sig")
                st.download_button(
                    label="ä¸‹è½½åˆ†æç»“æœCSV",
                    data=csv,
                    file_name="sentiment_analysis_results.csv",
                    mime="text/csv",
                )

    # Footer
    show_footer()


main()
